{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"# HuSpaCy is a spaCy library providing industrial-strength Hungarian language processing facilities through spaCy models. The released pipelines consist of a tokenizer, sentence splitter, lemmatizer, tagger (predicting morphological features as well), dependency parser and a named entity recognition module. Word and phrase embeddings are also available through spaCy's API. All models have high throughput, decent memory usage and close to state-of-the-art accuracy. A live demo is available here , model releases are published to Hugging Face Hub .","title":"Welcome"},{"location":"#_1","text":"HuSpaCy is a spaCy library providing industrial-strength Hungarian language processing facilities through spaCy models. The released pipelines consist of a tokenizer, sentence splitter, lemmatizer, tagger (predicting morphological features as well), dependency parser and a named entity recognition module. Word and phrase embeddings are also available through spaCy's API. All models have high throughput, decent memory usage and close to state-of-the-art accuracy. A live demo is available here , model releases are published to Hugging Face Hub .","title":""},{"location":"contact/","text":"Contact # For feature request issues and bugs please use the GitHub Issue Tracker . Otherwise, please use the Discussion Forums . Authors # HuSpaCy is implemented in the SzegedAI team, coordinated by Orosz Gy\u00f6rgy in the Hungarian AI National Laboratory, MILAB program.","title":"Contact us"},{"location":"contact/#contact","text":"For feature request issues and bugs please use the GitHub Issue Tracker . Otherwise, please use the Discussion Forums .","title":" Contact"},{"location":"contact/#authors","text":"HuSpaCy is implemented in the SzegedAI team, coordinated by Orosz Gy\u00f6rgy in the Hungarian AI National Laboratory, MILAB program.","title":"Authors"},{"location":"faq/","text":"Frequently asked questions # HuSpaCy is slow, what can I do? # Not it s not. :) You have several options to speed up your processing pipeline. If accuracy is not crucial use a smaller model: md < lg < trf Utilize GPU: use the following directive before loading the model. spacy . prefer_gpu () Batch processing of multiple documents are always faster. Use Language.pipe() method: texts = [ \"first doc\" , \"second doc\" ] docs = nlp . pipe ( texts ) Disable components not needed. When mining documents for named entities, the default model unnecessarily computes lemmata, PoS tags and dependency trees. You can easily disable them during model loading (c.f. spacy.load() or huspacy.load() ) or using Language.disable_pipe() nlp = huspacy . load ( \"hu_core_news_lg\" , disable = [ \"tagger\" ]) nlp . disable_pipe ( \"tagger\" ) The NER model usually confuses ORG and LOC entites, why is that? # The underlying model has been trained on corpora following the \"tag-for-meaning\" guideline which yields context dependent labels. For example referring to \"Budapest\" in the context of the Hungarian government should yield the ORG label while in other contexts it should be tagged as a LOC . Can I use HuSpaCy for my commercial software? # Yes, the tool is licensed under Apache 2.0 license, while all the models are CC BY-SA 4.0 .","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"","title":" Frequently asked questions"},{"location":"faq/#huspacy-is-slow-what-can-i-do","text":"Not it s not. :) You have several options to speed up your processing pipeline. If accuracy is not crucial use a smaller model: md < lg < trf Utilize GPU: use the following directive before loading the model. spacy . prefer_gpu () Batch processing of multiple documents are always faster. Use Language.pipe() method: texts = [ \"first doc\" , \"second doc\" ] docs = nlp . pipe ( texts ) Disable components not needed. When mining documents for named entities, the default model unnecessarily computes lemmata, PoS tags and dependency trees. You can easily disable them during model loading (c.f. spacy.load() or huspacy.load() ) or using Language.disable_pipe() nlp = huspacy . load ( \"hu_core_news_lg\" , disable = [ \"tagger\" ]) nlp . disable_pipe ( \"tagger\" )","title":"HuSpaCy is slow, what can I do?"},{"location":"faq/#the-ner-model-usually-confuses-org-and-loc-entites-why-is-that","text":"The underlying model has been trained on corpora following the \"tag-for-meaning\" guideline which yields context dependent labels. For example referring to \"Budapest\" in the context of the Hungarian government should yield the ORG label while in other contexts it should be tagged as a LOC .","title":"The NER model usually confuses ORG and LOC entites, why is that?"},{"location":"faq/#can-i-use-huspacy-for-my-commercial-software","text":"Yes, the tool is licensed under Apache 2.0 license, while all the models are CC BY-SA 4.0 .","title":"Can I use HuSpaCy for my commercial software?"},{"location":"examples/anonymizer/","text":"Text Anonymization # Using Presidio \u00b9 and Faker \u00b2 , we can easily make a simple text anonymization tool or PII (Personally Identifiable Information) removal tool. The (hu)spaCy integration of Presidio can be used to identify and remove personal data, such as names, locations, phone numbers, or even bank details. This tool uses (hu)spaCy's Named Entity Recognition facilities and further pattern matching rules. What is more, an easy-to-use de-identification method is provided by Faker as we show below. Initializing Presidio with HuSpaCy # Option 1: using only model names # // Here we use the hu_core_news_lg model , but any model supporting NER is a valid option configuration = { \"nlp_engine_name\" : \"spacy\" , \"models\" : [{ \"lang_code\" : \"hu\" , \"model_name\" : \"hu_core_news_lg\" , }], } provider = NlpEngineProvider ( nlp_configuration = configuration ) nlp_engine = provider . create_engine () analyzer = AnalyzerEngine ( nlp_engine = nlp_engine , supported_languages = [ \"hu\" ],) Option 2: Building on a previously initialized nlp # class HuSpaCyNlpEngine ( SpacyNlpEngine ): def __init__ ( self , nlp : Language ): self . nlp = { \"hu\" : nlp } def process (): nlp = spacy . load ( \"hu_core_news_trf\" ) nlp_engine = HuSpaCyNlpEngine ( nlp ) analyzer = AnalyzerEngine ( nlp_engine = nlp_engine , supported_languages = [ \"hu\" ]) Configuring Faker operators # fake = Faker ( locale = [ \"hu_HU\" ]) fake_operators = { \"PERSON\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . name ()}), \"LOCATION\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . address ()}), \"EMAIL_ADDRESS\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . email ()}), \"PHONE_NUMBER\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . phone_number ()}), \"CRYPTO\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . password ()}), \"IP_ADDRESS\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . ipv4 ()}), \"URL\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . url ()}), \"DATE_TIME\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . date ()}), \"CREDIT_CARD\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . credit_card_number ()}), \"IBAN_CODE\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . iban ()}), } Applying Presidio's anonymizer with Faker # anonymizer = AnonymizerEngine () anonymized_text = anonymizer . anonymize ( text = text , analyzer_results = results , operators = fake_operators ) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":"Anonymizer"},{"location":"examples/anonymizer/#text-anonymization","text":"Using Presidio \u00b9 and Faker \u00b2 , we can easily make a simple text anonymization tool or PII (Personally Identifiable Information) removal tool. The (hu)spaCy integration of Presidio can be used to identify and remove personal data, such as names, locations, phone numbers, or even bank details. This tool uses (hu)spaCy's Named Entity Recognition facilities and further pattern matching rules. What is more, an easy-to-use de-identification method is provided by Faker as we show below.","title":" Text Anonymization"},{"location":"examples/anonymizer/#initializing-presidio-with-huspacy","text":"","title":"Initializing Presidio with HuSpaCy"},{"location":"examples/anonymizer/#option-1-using-only-model-names","text":"// Here we use the hu_core_news_lg model , but any model supporting NER is a valid option configuration = { \"nlp_engine_name\" : \"spacy\" , \"models\" : [{ \"lang_code\" : \"hu\" , \"model_name\" : \"hu_core_news_lg\" , }], } provider = NlpEngineProvider ( nlp_configuration = configuration ) nlp_engine = provider . create_engine () analyzer = AnalyzerEngine ( nlp_engine = nlp_engine , supported_languages = [ \"hu\" ],)","title":"Option 1: using only model names"},{"location":"examples/anonymizer/#option-2-building-on-a-previously-initialized-nlp","text":"class HuSpaCyNlpEngine ( SpacyNlpEngine ): def __init__ ( self , nlp : Language ): self . nlp = { \"hu\" : nlp } def process (): nlp = spacy . load ( \"hu_core_news_trf\" ) nlp_engine = HuSpaCyNlpEngine ( nlp ) analyzer = AnalyzerEngine ( nlp_engine = nlp_engine , supported_languages = [ \"hu\" ])","title":"Option 2: Building on a previously initialized nlp"},{"location":"examples/anonymizer/#configuring-faker-operators","text":"fake = Faker ( locale = [ \"hu_HU\" ]) fake_operators = { \"PERSON\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . name ()}), \"LOCATION\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . address ()}), \"EMAIL_ADDRESS\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . email ()}), \"PHONE_NUMBER\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . phone_number ()}), \"CRYPTO\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . password ()}), \"IP_ADDRESS\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . ipv4 ()}), \"URL\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . url ()}), \"DATE_TIME\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . date ()}), \"CREDIT_CARD\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . credit_card_number ()}), \"IBAN_CODE\" : OperatorConfig ( \"custom\" , { \"lambda\" : lambda x : fake . iban ()}), }","title":"Configuring Faker operators"},{"location":"examples/anonymizer/#applying-presidios-anonymizer-with-faker","text":"anonymizer = AnonymizerEngine () anonymized_text = anonymizer . anonymize ( text = text , analyzer_results = results , operators = fake_operators ) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":"Applying Presidio's anonymizer with Faker"},{"location":"examples/dbpedia/","text":"Entity Linking # Using the DBpedia Spotlight integration for SpaCy \u00b9 , we can easily extract and link DBpedia entities from any text. Initalizing DBpedia Spotlight # nlp = spacy . load ( \"hu_core_news_trf\" ) nlp . add_pipe ( \"dbpedia_spotlight\" , config = { 'dbpedia_rest_endpoint' : 'https://dbpedia-spotlight.dsd.sztaki.hu/hu' , 'overwrite_ents' : False }) Presenting the entities # doc = nlp ( \"A M\u00e1trix c\u00edm\u0171 sci-fi film Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Joe Pantoliano \u00e9s Hugo Weaving f\u0151szerepl\u00e9s\u00e9vel.\" ) pd . DataFrame ([ { \"Text\" : ent . text , \"Resource\" : ent . kb_id_ , \"Similarity Score\" : ent . _ . dbpedia_raw_result [ '@similarityScore' ]} for ent in doc . spans [ \"dbpedia_spotlight\" ]]) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":"Entity linking"},{"location":"examples/dbpedia/#entity-linking","text":"Using the DBpedia Spotlight integration for SpaCy \u00b9 , we can easily extract and link DBpedia entities from any text.","title":" Entity Linking"},{"location":"examples/dbpedia/#initalizing-dbpedia-spotlight","text":"nlp = spacy . load ( \"hu_core_news_trf\" ) nlp . add_pipe ( \"dbpedia_spotlight\" , config = { 'dbpedia_rest_endpoint' : 'https://dbpedia-spotlight.dsd.sztaki.hu/hu' , 'overwrite_ents' : False })","title":"Initalizing DBpedia Spotlight"},{"location":"examples/dbpedia/#presenting-the-entities","text":"doc = nlp ( \"A M\u00e1trix c\u00edm\u0171 sci-fi film Keanu Reeves, Laurence Fishburne, Carrie-Anne Moss, Joe Pantoliano \u00e9s Hugo Weaving f\u0151szerepl\u00e9s\u00e9vel.\" ) pd . DataFrame ([ { \"Text\" : ent . text , \"Resource\" : ent . kb_id_ , \"Similarity Score\" : ent . _ . dbpedia_raw_result [ '@similarityScore' ]} for ent in doc . spans [ \"dbpedia_spotlight\" ]]) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":"Presenting the entities"},{"location":"examples/keyphrase/","text":"Keyphrase Extraction # Keyphrase extraction is a well-studied problem of natural language processing, thus many ready-made solutions exist. textacy is higher-level NLP library (built on spaCy) implementing several keyword extraction methods. By using this tool, we can easily build a simple solution for this problem. First, you need to load a HuSpaCy model, and process the text you wish to analyze: import huspacy nlp = huspacy . load () doc = nlp ( text ) Then, you need to decide which key term extraction method should be utilized, as textacy implements several ones . For the sake of simplicity we rely on SGRank and fine-tune it through PoS and word n-gram filters. from textacy.extract.keyterms.sgrank import sgrank as keywords terms : List [ Tuple [ str , float ]] = keywords ( doc , topn = 10 , include_pos = ( \"NOUN\" , \"PROPN\" ), ngrams = ( 1 , 2 , 3 )) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":"Keyphrase extraction"},{"location":"examples/keyphrase/#keyphrase-extraction","text":"Keyphrase extraction is a well-studied problem of natural language processing, thus many ready-made solutions exist. textacy is higher-level NLP library (built on spaCy) implementing several keyword extraction methods. By using this tool, we can easily build a simple solution for this problem. First, you need to load a HuSpaCy model, and process the text you wish to analyze: import huspacy nlp = huspacy . load () doc = nlp ( text ) Then, you need to decide which key term extraction method should be utilized, as textacy implements several ones . For the sake of simplicity we rely on SGRank and fine-tune it through PoS and word n-gram filters. from textacy.extract.keyterms.sgrank import sgrank as keywords terms : List [ Tuple [ str , float ]] = keywords ( doc , topn = 10 , include_pos = ( \"NOUN\" , \"PROPN\" ), ngrams = ( 1 , 2 , 3 )) This example is available on Hugging Face Spaces , while the full source code is on GitHub .","title":" Keyphrase Extraction"},{"location":"examples/relation/","text":"Relation Extraction # By using a set of simplerules, we can extract an ordered sequence of subject-verb-object triples from a document or sentence. For example, in the sentence \"Anna \u00e9ppen h\u00e1zat \u00e9p\u00edt mag\u00e1nak.\" (Anna builds a house for herself.) \"Anna\" will be the subject, \"\u00e9p\u00edt\" will be the verb, and the \"h\u00e1z\" will be the object. This triple will be an extracted relation. Using HuSpaCy for tagging and dependency parsing and the modified code for extraction, we can make a simple relaction extraction tool. Importing and using HuSpaCy # nlp = spacy . load ( \"hu_core_news_trf\" ) doc = nlp ( \"Anna \u00e9ppen h\u00e1zat \u00e9p\u00edt mag\u00e1nak.\" ) Extraction SVO triples # def subject_verb_object_triples ( doclike : types . DocLike ) -> Iterable [ SVOTriple ]: if isinstance ( doclike , Span ): sents = [ doclike ] else : sents = doclike . sents for sent in sents : # connect subjects/objects to direct verb heads # and expand them to include conjuncts, compound nouns, ... verb_sos = collections . defaultdict ( lambda : collections . defaultdict ( set )) for tok in sent : head = tok . head # ensure entry for all verbs, even if empty # to catch conjugate verbs without direct subject/object deps if tok . pos == VERB : _ = verb_sos [ tok ] # nominal subject of active or passive verb if tok . dep in _NOMINAL_SUBJ_DEPS : if head . pos == VERB : verb_sos [ head ][ \"subjects\" ] . update ( expand_noun ( tok )) # clausal subject of active or passive verb elif tok . dep in _CLAUSAL_SUBJ_DEPS : if head . pos == VERB : verb_sos [ head ][ \"subjects\" ] . update ( tok . subtree ) # nominal direct object of transitive verb elif tok . dep == obj : if head . pos == VERB : verb_sos [ head ][ \"objects\" ] . update ( expand_noun ( tok )) # prepositional object acting as agent of passive verb elif tok . dep == pobj : if head . dep == agent and head . head . pos == VERB : verb_sos [ head . head ][ \"objects\" ] . update ( expand_noun ( tok )) # open clausal complement, but not as a secondary predicate elif tok . dep == xcomp : if ( head . pos == VERB and not any ( child . dep == obj for child in head . children ) ): # TODO: just the verb, or the whole tree? # verb_sos[verb][\"objects\"].update(expand_verb(tok)) verb_sos [ head ][ \"objects\" ] . update ( tok . subtree ) # fill in any indirect relationships connected via verb conjuncts for verb , so_dict in verb_sos . items (): conjuncts = verb . conjuncts if so_dict . get ( \"subjects\" ): for conj in conjuncts : conj_so_dict = verb_sos . get ( conj ) if conj_so_dict and not conj_so_dict . get ( \"subjects\" ): conj_so_dict [ \"subjects\" ] . update ( so_dict [ \"subjects\" ]) if not so_dict . get ( \"objects\" ): so_dict [ \"objects\" ] . update ( obj for conj in conjuncts for obj in verb_sos . get ( conj , {}) . get ( \"objects\" , []) ) # expand verbs and restructure into svo triples for verb , so_dict in verb_sos . items (): if so_dict [ \"subjects\" ] and so_dict [ \"objects\" ]: yield SVOTriple ( subject = sorted ( so_dict [ \"subjects\" ], key = attrgetter ( \"i\" )), verb = sorted ( expand_verb ( verb ), key = attrgetter ( \"i\" )), object = sorted ( so_dict [ \"objects\" ], key = attrgetter ( \"i\" )), ) The function with all it's dependencies is available on GitHub . Extracting the triples # tuples = subject_verb_object_triples ( doc ) Presenting the data # for sub_multiple in tuples [ 0 ][ 0 ]: subject += str ( sub_multiple ) + \", \" subject = subject [: - 2 ] for verb_multiple in tuples [ 0 ][ 1 ]: verb += str ( verb_multiple ) + \", \" verb = verb [: - 2 ] for obj_multiple in tuples [ 0 ][ 2 ]: object += str ( obj_multiple ) + \", \" object = object [: - 2 ] relation_list = [[ subject , verb , object ]] df = pd . DataFrame ( relation_list , columns = [ 'Subject' , 'Verb' , 'Object' ]) Notes # The method presented above is heavily based on Textacy \u00b9 's similar method. We slightly modified to adapt for Hungarian. You can find the full extraction method here . This example is available on Hugging Face Spaces . The full source code is on GitHub .","title":"Relation extraction"},{"location":"examples/relation/#relation-extraction","text":"By using a set of simplerules, we can extract an ordered sequence of subject-verb-object triples from a document or sentence. For example, in the sentence \"Anna \u00e9ppen h\u00e1zat \u00e9p\u00edt mag\u00e1nak.\" (Anna builds a house for herself.) \"Anna\" will be the subject, \"\u00e9p\u00edt\" will be the verb, and the \"h\u00e1z\" will be the object. This triple will be an extracted relation. Using HuSpaCy for tagging and dependency parsing and the modified code for extraction, we can make a simple relaction extraction tool.","title":" Relation Extraction"},{"location":"examples/relation/#importing-and-using-huspacy","text":"nlp = spacy . load ( \"hu_core_news_trf\" ) doc = nlp ( \"Anna \u00e9ppen h\u00e1zat \u00e9p\u00edt mag\u00e1nak.\" )","title":"Importing and using HuSpaCy"},{"location":"examples/relation/#extraction-svo-triples","text":"def subject_verb_object_triples ( doclike : types . DocLike ) -> Iterable [ SVOTriple ]: if isinstance ( doclike , Span ): sents = [ doclike ] else : sents = doclike . sents for sent in sents : # connect subjects/objects to direct verb heads # and expand them to include conjuncts, compound nouns, ... verb_sos = collections . defaultdict ( lambda : collections . defaultdict ( set )) for tok in sent : head = tok . head # ensure entry for all verbs, even if empty # to catch conjugate verbs without direct subject/object deps if tok . pos == VERB : _ = verb_sos [ tok ] # nominal subject of active or passive verb if tok . dep in _NOMINAL_SUBJ_DEPS : if head . pos == VERB : verb_sos [ head ][ \"subjects\" ] . update ( expand_noun ( tok )) # clausal subject of active or passive verb elif tok . dep in _CLAUSAL_SUBJ_DEPS : if head . pos == VERB : verb_sos [ head ][ \"subjects\" ] . update ( tok . subtree ) # nominal direct object of transitive verb elif tok . dep == obj : if head . pos == VERB : verb_sos [ head ][ \"objects\" ] . update ( expand_noun ( tok )) # prepositional object acting as agent of passive verb elif tok . dep == pobj : if head . dep == agent and head . head . pos == VERB : verb_sos [ head . head ][ \"objects\" ] . update ( expand_noun ( tok )) # open clausal complement, but not as a secondary predicate elif tok . dep == xcomp : if ( head . pos == VERB and not any ( child . dep == obj for child in head . children ) ): # TODO: just the verb, or the whole tree? # verb_sos[verb][\"objects\"].update(expand_verb(tok)) verb_sos [ head ][ \"objects\" ] . update ( tok . subtree ) # fill in any indirect relationships connected via verb conjuncts for verb , so_dict in verb_sos . items (): conjuncts = verb . conjuncts if so_dict . get ( \"subjects\" ): for conj in conjuncts : conj_so_dict = verb_sos . get ( conj ) if conj_so_dict and not conj_so_dict . get ( \"subjects\" ): conj_so_dict [ \"subjects\" ] . update ( so_dict [ \"subjects\" ]) if not so_dict . get ( \"objects\" ): so_dict [ \"objects\" ] . update ( obj for conj in conjuncts for obj in verb_sos . get ( conj , {}) . get ( \"objects\" , []) ) # expand verbs and restructure into svo triples for verb , so_dict in verb_sos . items (): if so_dict [ \"subjects\" ] and so_dict [ \"objects\" ]: yield SVOTriple ( subject = sorted ( so_dict [ \"subjects\" ], key = attrgetter ( \"i\" )), verb = sorted ( expand_verb ( verb ), key = attrgetter ( \"i\" )), object = sorted ( so_dict [ \"objects\" ], key = attrgetter ( \"i\" )), ) The function with all it's dependencies is available on GitHub .","title":"Extraction SVO triples"},{"location":"examples/relation/#extracting-the-triples","text":"tuples = subject_verb_object_triples ( doc )","title":"Extracting the triples"},{"location":"examples/relation/#presenting-the-data","text":"for sub_multiple in tuples [ 0 ][ 0 ]: subject += str ( sub_multiple ) + \", \" subject = subject [: - 2 ] for verb_multiple in tuples [ 0 ][ 1 ]: verb += str ( verb_multiple ) + \", \" verb = verb [: - 2 ] for obj_multiple in tuples [ 0 ][ 2 ]: object += str ( obj_multiple ) + \", \" object = object [: - 2 ] relation_list = [[ subject , verb , object ]] df = pd . DataFrame ( relation_list , columns = [ 'Subject' , 'Verb' , 'Object' ])","title":"Presenting the data"},{"location":"examples/relation/#notes","text":"The method presented above is heavily based on Textacy \u00b9 's similar method. We slightly modified to adapt for Hungarian. You can find the full extraction method here . This example is available on Hugging Face Spaces . The full source code is on GitHub .","title":"Notes"},{"location":"huspacy/changelog/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . [UNRELEASED] # Changed # Dropped Python 3.6 support 0.5 # Components # trainable_lemmatizer_v2 : fork LemmaSmoother for improving lemmatization output of the trainable_lemmatizer RomanToArabic for convert Roman numbers to Arabic ones LookupLemmatizer to memoize token,pos -> lemma transformations Fixed # Fixing #46 : huspacy core no longer depends on spaCy Refactored components Added # Helper method for listing models available download() first validates the model before downloading it 0.4.3 # Fixed # Documentation updates Fixed model loading 0.4.2 # Initial release # Convenience functions for downloading and loading models","title":"Changelog"},{"location":"huspacy/changelog/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"huspacy/changelog/#unreleased","text":"","title":"[UNRELEASED]"},{"location":"huspacy/changelog/#changed","text":"Dropped Python 3.6 support","title":"Changed"},{"location":"huspacy/changelog/#05","text":"","title":"0.5"},{"location":"huspacy/changelog/#components","text":"trainable_lemmatizer_v2 : fork LemmaSmoother for improving lemmatization output of the trainable_lemmatizer RomanToArabic for convert Roman numbers to Arabic ones LookupLemmatizer to memoize token,pos -> lemma transformations","title":"Components"},{"location":"huspacy/changelog/#fixed","text":"Fixing #46 : huspacy core no longer depends on spaCy Refactored components","title":"Fixed"},{"location":"huspacy/changelog/#added","text":"Helper method for listing models available download() first validates the model before downloading it","title":"Added"},{"location":"huspacy/changelog/#043","text":"","title":"0.4.3"},{"location":"huspacy/changelog/#fixed_1","text":"Documentation updates Fixed model loading","title":"Fixed"},{"location":"huspacy/changelog/#042","text":"","title":"0.4.2"},{"location":"huspacy/changelog/#initial-release","text":"Convenience functions for downloading and loading models","title":"Initial release"},{"location":"huspacy/contributing/","text":"Contributing #","title":":octicons-git-pull-request-24: Contributing"},{"location":"huspacy/contributing/#contributing","text":"","title":" Contributing"},{"location":"huspacy/installation/","text":"Installation # To get started using the tool, first, we need to download one of the models. The easiest way to achieve this is to install huspacy (from PyPI ) and then fetch a model through its API. pip install huspacy import huspacy # Download the latest CPU optimized model huspacy . download () Install the models directly # You can install the latest models directly from \ud83e\udd17 Hugging Face Hub: CPU optimized large model : pip install https://huggingface.co/huspacy/hu_core_news_lg/resolve/main/hu_core_news_lg-any-py3-none-any.whl GPU optimized transformers model : pip install https://huggingface.co/huspacy/hu_core_news_trf/resolve/main/hu_core_news_trf-any-py3-none-any.whl To speed up inference on GPUs, CUDA should be installed as described in https://spacy.io/usage .","title":"Installation"},{"location":"huspacy/installation/#installation","text":"To get started using the tool, first, we need to download one of the models. The easiest way to achieve this is to install huspacy (from PyPI ) and then fetch a model through its API. pip install huspacy import huspacy # Download the latest CPU optimized model huspacy . download ()","title":" Installation"},{"location":"huspacy/installation/#install-the-models-directly","text":"You can install the latest models directly from \ud83e\udd17 Hugging Face Hub: CPU optimized large model : pip install https://huggingface.co/huspacy/hu_core_news_lg/resolve/main/hu_core_news_lg-any-py3-none-any.whl GPU optimized transformers model : pip install https://huggingface.co/huspacy/hu_core_news_trf/resolve/main/hu_core_news_trf-any-py3-none-any.whl To speed up inference on GPUs, CUDA should be installed as described in https://spacy.io/usage .","title":"Install the models directly"},{"location":"huspacy/quickstart/","text":"# Quicstart # HuSpaCy is fully compatible with spaCy's API , newcomers can easily get started with spaCy 101 guide. Although HuSpacy models can be loaded with spacy.load(...) , the tool provides convenience methods to easily access downloaded models. # Load the model using spacy.load(...) import spacy nlp = spacy . load ( \"hu_core_news_lg\" ) # Load the default large model (if downloaded) import huspacy nlp = huspacy . load () # Load the model directly as a module import hu_core_news_lg nlp = hu_core_news_lg . load () # Process texts doc = nlp ( \"Csiribiri csiribiri zabszalma - n\u00e9gy csillag k\u00f6zt alszom ma.\" )","title":"Quickstart"},{"location":"huspacy/quickstart/#quicstart","text":"HuSpaCy is fully compatible with spaCy's API , newcomers can easily get started with spaCy 101 guide. Although HuSpacy models can be loaded with spacy.load(...) , the tool provides convenience methods to easily access downloaded models. # Load the model using spacy.load(...) import spacy nlp = spacy . load ( \"hu_core_news_lg\" ) # Load the default large model (if downloaded) import huspacy nlp = huspacy . load () # Load the model directly as a module import hu_core_news_lg nlp = hu_core_news_lg . load () # Process texts doc = nlp ( \"Csiribiri csiribiri zabszalma - n\u00e9gy csillag k\u00f6zt alszom ma.\" )","title":"#  Quicstart"},{"location":"huspacy/under_the_hood/","text":"Under the hood # HuSpaCy is built on top of spaCy hence almost all components are part of that NLP framework. Explosion's YouTube channel and Blog has several excellent tutorials on the underlying methods. Talks, publications # We regularly publish our results and talk on conferences and meetups. All materials are collected in this public repository .","title":"Under the hood"},{"location":"huspacy/under_the_hood/#under-the-hood","text":"HuSpaCy is built on top of spaCy hence almost all components are part of that NLP framework. Explosion's YouTube channel and Blog has several excellent tutorials on the underlying methods.","title":" Under the hood"},{"location":"huspacy/under_the_hood/#talks-publications","text":"We regularly publish our results and talk on conferences and meetups. All materials are collected in this public repository .","title":"Talks, publications"},{"location":"huspacy/usage_in_r/","text":"Usage in R # TBD","title":"Usage in R"},{"location":"huspacy/usage_in_r/#usage-in-r","text":"TBD","title":" Usage in R"},{"location":"models/","text":"Models overview # We provide several pretrained models, the ( hu_core_news_lg ) one is a CNN-based large model which achieves a good balance between accuracy and processing speed. This default model provides tokenization, sentence splitting, part-of-speech tagging (UD labels w/ detailed morphosyntactic features), lemmatization, dependency parsing and named entity recognition and ships with pretrained word vectors. The second model ( hu_core_news_trf ) is built on huBERT and provides the same functionality as the large model except the word vectors. It comes with much higher accuracy in the price of increased computational resource usage. We suggest using it with GPU support. The hu_core_news_md pipeline greatly improves on hu_core_news_lg 's throughput by loosing some accuracy. This model could be a good choice when processing speed is crucial. A demo of these models is available at Hugging Face Spaces . Comparison # Models md lg trf Embeddings 100d floret 300d floret transformer: huBert Target hardwer CPU CPU GPU Accuracy Resource usage","title":"Overview"},{"location":"models/#models-overview","text":"We provide several pretrained models, the ( hu_core_news_lg ) one is a CNN-based large model which achieves a good balance between accuracy and processing speed. This default model provides tokenization, sentence splitting, part-of-speech tagging (UD labels w/ detailed morphosyntactic features), lemmatization, dependency parsing and named entity recognition and ships with pretrained word vectors. The second model ( hu_core_news_trf ) is built on huBERT and provides the same functionality as the large model except the word vectors. It comes with much higher accuracy in the price of increased computational resource usage. We suggest using it with GPU support. The hu_core_news_md pipeline greatly improves on hu_core_news_lg 's throughput by loosing some accuracy. This model could be a good choice when processing speed is crucial. A demo of these models is available at Hugging Face Spaces .","title":" Models overview"},{"location":"models/#comparison","text":"Models md lg trf Embeddings 100d floret 300d floret transformer: huBert Target hardwer CPU CPU GPU Accuracy Resource usage","title":"Comparison"},{"location":"models/changelog_lg/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . 3.4.2 # Changed # Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) Improved lemmatization Replaced spacy's trainable_lemmatizer with our own port of it ( trainable_lemmatizer_v2 ) which improves on the lemmatization of uppercase tokens Added lemma smoothing step for improving on common errors of the trainable lemmatizer 3.4.1 # Changed # Improved lookup lemmatizer: bugfix and morph. feats usage for indexing lemma 3.4.0 # Changed # spaCy 3.4.x compatibility improved tagging performance 3.3.1 # Added # Added lookup lemmatizer before edit tree lemmatizer in the pipeline Added lemma smoother after edit tree lemmatizer in the pipeline 3.3.0 # Changed # Replaced Lemmy lemmatizer w/ edit tree lemmatizer 3.2.2 # Changed # Replaced static word vectors w/ char n-gram based floret ones Added multistep training to mitigate non-deterministic training behaviour 3.2.1 # Changed # Using the Szarvas-Farkas split for SzegedNER Learning lemmata w/o \"+\" characters hunnerwiki is no longer used to train the NER Started using spacy's model numbering convention 0.4.2 # Fixed # Better integration of the lemmatizer Updated the project's documentation 0.4.1 # Added # Changed # NER model is built on NerKor and SzegedNER Improved lemmatization for numbers and sentence starting tokens Improved lemmatization by using the whole Szeged Corpus Improved PoS tagging by pretraining on a silver standard corpora Improved Dependency parser by using pretraining on silver standard corpora Improved sentence splitter by using the multitask neural model Fixed # Compatibility w/ Spacy 3.x 0.3.1 - 2019-10-03 # Fixed # Compatibility w/ Spacy 2.2.x 0.3.0 - 2019-09-26 # Added # Named Entity Recognition 0.2.0 - 2019-06-02 # Fixed # Compatibility w/ Spacy 2.1.x Added # Minor sentence segmentation improvements Minor improvements in PoS tagging 0.1.0 - 2019-01-04 # Added # Lemmatization support using lemmy Rule based lemmatizer Multi-task CNN-based dependency parser Changed # Support for spaCy 2.x Using the UD Hungarian corpus for the whole training process 0.0.1 - 2017-06-11 # Added # Experimental support for spaCy 1.x PoS Tagger model with word vectors trained on an unreleased automatically transcribed Szeged Korpusz version","title":"hu_core_news_lg"},{"location":"models/changelog_lg/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"models/changelog_lg/#342","text":"","title":"3.4.2"},{"location":"models/changelog_lg/#changed","text":"Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) Improved lemmatization Replaced spacy's trainable_lemmatizer with our own port of it ( trainable_lemmatizer_v2 ) which improves on the lemmatization of uppercase tokens Added lemma smoothing step for improving on common errors of the trainable lemmatizer","title":"Changed"},{"location":"models/changelog_lg/#341","text":"","title":"3.4.1"},{"location":"models/changelog_lg/#changed_1","text":"Improved lookup lemmatizer: bugfix and morph. feats usage for indexing lemma","title":"Changed"},{"location":"models/changelog_lg/#340","text":"","title":"3.4.0"},{"location":"models/changelog_lg/#changed_2","text":"spaCy 3.4.x compatibility improved tagging performance","title":"Changed"},{"location":"models/changelog_lg/#331","text":"","title":"3.3.1"},{"location":"models/changelog_lg/#added","text":"Added lookup lemmatizer before edit tree lemmatizer in the pipeline Added lemma smoother after edit tree lemmatizer in the pipeline","title":"Added"},{"location":"models/changelog_lg/#330","text":"","title":"3.3.0"},{"location":"models/changelog_lg/#changed_3","text":"Replaced Lemmy lemmatizer w/ edit tree lemmatizer","title":"Changed"},{"location":"models/changelog_lg/#322","text":"","title":"3.2.2"},{"location":"models/changelog_lg/#changed_4","text":"Replaced static word vectors w/ char n-gram based floret ones Added multistep training to mitigate non-deterministic training behaviour","title":"Changed"},{"location":"models/changelog_lg/#321","text":"","title":"3.2.1"},{"location":"models/changelog_lg/#changed_5","text":"Using the Szarvas-Farkas split for SzegedNER Learning lemmata w/o \"+\" characters hunnerwiki is no longer used to train the NER Started using spacy's model numbering convention","title":"Changed"},{"location":"models/changelog_lg/#042","text":"","title":"0.4.2"},{"location":"models/changelog_lg/#fixed","text":"Better integration of the lemmatizer Updated the project's documentation","title":"Fixed"},{"location":"models/changelog_lg/#041","text":"","title":"0.4.1"},{"location":"models/changelog_lg/#added_1","text":"","title":"Added"},{"location":"models/changelog_lg/#changed_6","text":"NER model is built on NerKor and SzegedNER Improved lemmatization for numbers and sentence starting tokens Improved lemmatization by using the whole Szeged Corpus Improved PoS tagging by pretraining on a silver standard corpora Improved Dependency parser by using pretraining on silver standard corpora Improved sentence splitter by using the multitask neural model","title":"Changed"},{"location":"models/changelog_lg/#fixed_1","text":"Compatibility w/ Spacy 3.x","title":"Fixed"},{"location":"models/changelog_lg/#031-2019-10-03","text":"","title":"0.3.1 - 2019-10-03"},{"location":"models/changelog_lg/#fixed_2","text":"Compatibility w/ Spacy 2.2.x","title":"Fixed"},{"location":"models/changelog_lg/#030-2019-09-26","text":"","title":"0.3.0 - 2019-09-26"},{"location":"models/changelog_lg/#added_2","text":"Named Entity Recognition","title":"Added"},{"location":"models/changelog_lg/#020-2019-06-02","text":"","title":"0.2.0 - 2019-06-02"},{"location":"models/changelog_lg/#fixed_3","text":"Compatibility w/ Spacy 2.1.x","title":"Fixed"},{"location":"models/changelog_lg/#added_3","text":"Minor sentence segmentation improvements Minor improvements in PoS tagging","title":"Added"},{"location":"models/changelog_lg/#010-2019-01-04","text":"","title":"0.1.0 - 2019-01-04"},{"location":"models/changelog_lg/#added_4","text":"Lemmatization support using lemmy Rule based lemmatizer Multi-task CNN-based dependency parser","title":"Added"},{"location":"models/changelog_lg/#changed_7","text":"Support for spaCy 2.x Using the UD Hungarian corpus for the whole training process","title":"Changed"},{"location":"models/changelog_lg/#001-2017-06-11","text":"","title":"0.0.1 - 2017-06-11"},{"location":"models/changelog_lg/#added_5","text":"Experimental support for spaCy 1.x PoS Tagger model with word vectors trained on an unreleased automatically transcribed Szeged Korpusz version","title":"Added"},{"location":"models/changelog_md/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . 3.4.1 - 2022-10-07 # Initial release # Added medium model, with medium (100d) vectors based on Webcorpus 2.0 Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) Improved lemmatization Replaced spacy's trainable_lemmatizer with our own port of it ( trainable_lemmatizer_v2 ) which improves on the lemmatization of uppercase tokens Added lemma smoothing step for improving on common errors of the trainable lemmatizer","title":"hu_core_news_md"},{"location":"models/changelog_md/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"models/changelog_md/#341-2022-10-07","text":"","title":"3.4.1 - 2022-10-07"},{"location":"models/changelog_md/#initial-release","text":"Added medium model, with medium (100d) vectors based on Webcorpus 2.0 Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) Improved lemmatization Replaced spacy's trainable_lemmatizer with our own port of it ( trainable_lemmatizer_v2 ) which improves on the lemmatization of uppercase tokens Added lemma smoothing step for improving on common errors of the trainable lemmatizer","title":"Initial release"},{"location":"models/changelog_trf/","text":"Changelog # All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning . 3.2.5 # Changed # Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) spacy 3.4.x compatibility Updated spacy-experimental (including the biaffine parser) dependency 3.2.4 # Changed # Updated dependencies 3.2.3 # Changed # Improved lookup lemmatizer: bugfix and morph. feats usage for indexing lemma 3.2.2 # Added # Added lookup lemmatizer before edit tree lemmatizer in the pipeline Added lemma smoother after edit tree lemmatizer in the pipeline 3.2.1 # Changed # Minor improvements in the training pipeline 3.2.0 # Added # Transformer encoder Experimental edit-tree-lemmatizer Experimental biaffine parser Using the Szarvas-Farkas split for SzegedNER Learning lemmata w/o \"+\" characters hunnerwiki is no longer used to train the NER Started using spacy's model numbering convention","title":"hu_core_news_trf"},{"location":"models/changelog_trf/#changelog","text":"All notable changes to this project will be documented in this file. The format is based on Keep a Changelog , and this project adheres to Semantic Versioning .","title":"Changelog"},{"location":"models/changelog_trf/#325","text":"","title":"3.2.5"},{"location":"models/changelog_trf/#changed","text":"Improved NER: using beam_ner with update_prob=1.0 Using the r2.10 version on UD corpus ( Github Changelog ) Using the v3 version on Szeged-Corpus ( Github Changelog ) spacy 3.4.x compatibility Updated spacy-experimental (including the biaffine parser) dependency","title":"Changed"},{"location":"models/changelog_trf/#324","text":"","title":"3.2.4"},{"location":"models/changelog_trf/#changed_1","text":"Updated dependencies","title":"Changed"},{"location":"models/changelog_trf/#323","text":"","title":"3.2.3"},{"location":"models/changelog_trf/#changed_2","text":"Improved lookup lemmatizer: bugfix and morph. feats usage for indexing lemma","title":"Changed"},{"location":"models/changelog_trf/#322","text":"","title":"3.2.2"},{"location":"models/changelog_trf/#added","text":"Added lookup lemmatizer before edit tree lemmatizer in the pipeline Added lemma smoother after edit tree lemmatizer in the pipeline","title":"Added"},{"location":"models/changelog_trf/#321","text":"","title":"3.2.1"},{"location":"models/changelog_trf/#changed_3","text":"Minor improvements in the training pipeline","title":"Changed"},{"location":"models/changelog_trf/#320","text":"","title":"3.2.0"},{"location":"models/changelog_trf/#added_1","text":"Transformer encoder Experimental edit-tree-lemmatizer Experimental biaffine parser Using the Szarvas-Farkas split for SzegedNER Learning lemmata w/o \"+\" characters hunnerwiki is no longer used to train the NER Started using spacy's model numbering convention","title":"Added"}]}